{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kelime Sınıflandırıcı\n",
    "\n",
    "Bu proje, verilen bir kelimenin hangi dilde olmaya daha yatkın olduğunu tahmin etmek için LSTM (long short term memory) denen derin öğrenme metodonu kullanıyor.\n",
    "\n",
    "Veri olarak 100.000 adet, her iki dilden de 50.000 adet kelime kullanıldı.\n",
    "\n",
    "Türkçe'ye özel karakterler olan ü, ö, ı, ç, ş gibi harfleri en yakın genel latin karşılığına çevirerek kullandım. Bunun yanında, İngilizce'de bu karakterler nispeten daha az olduğundan, İngilize kelimelerdeki x, w ve q karakterlerine dokunmadım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullanılan Modüller\n",
    "\n",
    "* Keras: YSA matematik modellemesi\n",
    "* Numpy: Veri işleme\n",
    "* Random: Listeleri karistirmak icin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veriyi Programa Aktarma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dosyadan Listeye\n",
    "\n",
    "Kelimeleri etiketleriyle birilkte data listesine ekliyorum.\n",
    "\n",
    "Sinif etiketlerini belirtmesi icin 0 ve 1 kullandim.\n",
    "\n",
    "Öğrenme sırasında verilerin sıralı düzen içinde verilmesi modelin optimizasyonunun kötü olmasına sebebiyet verebilir. Dolayısıyla listeyi karıştırmakta fayda var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open('turkish.txt') as textfile:\n",
    "    for word in textfile:\n",
    "        data.append((word.replace('\\n', ''), 0))\n",
    "\n",
    "with open('english.txt') as textfile:\n",
    "    for word in textfile:\n",
    "        data.append((word.replace('\\n', ''), 1))\n",
    "\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Öncü Veri Taraması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karakter havuzu: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z\n",
      "En uzun kelime: trinitrophenylmethylnitramine\n",
      "En uzun kelimenin uzunlugu: 29\n",
      "Kelime adedi: 99957\n"
     ]
    }
   ],
   "source": [
    "words = [record[0] for record in data]\n",
    "labels = [record[1] for record in data]\n",
    "\n",
    "char_pool = sorted(set(''.join(words)))\n",
    "longest = sorted(words, key=len)[-1]\n",
    "maxlen = len(longest)\n",
    "word_count = len(data)\n",
    "\n",
    "# Turkce ve ingilizce.\n",
    "n_classes = 2\n",
    "\n",
    "print('Karakter havuzu: {}'.format(\", \".join(char_pool)))\n",
    "print('En uzun kelime: {}'.format(longest))\n",
    "print('En uzun kelimenin uzunlugu: {}'.format(maxlen))\n",
    "print('Kelime adedi: {}'.format(word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ön İşleme\n",
    "\n",
    "Tüm karakterlere bir index atıyorum (ve vice-versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(char_pool))\n",
    "indices_char = dict((i, c) for i, c in enumerate(char_pool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kelime/Karakter tipindeki veriyi sayısal diziler şekline getiriyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her harf icin maxlen uzunlugunda 0'lar ile dolu bir vektor\n",
    "# Her kelime icin icinde o listelerin oldugu baska bir liste\n",
    "# Tum veriler icin de kelimelere ait listeleri barindiran ana bir liste\n",
    "# olusturacak sekilde 3 boyutlu bos bir dizi tanimlaniyor.\n",
    "x_data = np.zeros((word_count, maxlen, len(char_pool)), dtype=np.bool)\n",
    "\n",
    "# Toplam kelime sayisi ve cikti sayisina gore bir liste tanimlaniyor.\n",
    "y_data = np.zeros((word_count, n_classes))\n",
    "\n",
    "# [0, 0, ..., 0] seklinde olan karakter vektorundeki bir degeri karak-\n",
    "# terin index'ine gore 1 yapiyor. Dolayisiyla en sonunda elde bir ke-\n",
    "# lime icin sirayla dizilmis one-hot diziler kaliyor.\n",
    "for i_word, word in enumerate(words):\n",
    "    for i_char, char in enumerate(word):\n",
    "        x_data[i_word, i_char, char_indices[char]] = 1\n",
    "\n",
    "# [0, 0] olan etiket listesini duruma gore [0, 1] veya [1, 0] haline\n",
    "# getiriyor.\n",
    "for i_label, label in enumerate(labels):\n",
    "    y_data[i_label, label] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeli Yaratma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(16, input_shape=(maxlen, len(char_pool))))\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# geri donuslu yapay aglar icin genelde kullanilan optimizer\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ogrenim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "for iteration in range(3):\n",
    "    model.fit(x_data, y_data, batch_size=128, nb_epoch=1)\n",
    "print('Ogrenim tamamlandi!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deneme!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(word):\n",
    "    '''Verilen kelimenin dillere gore aitlik olasiliklarini hesaplar.'''\n",
    "    processed_word = np.zeros((1, maxlen, len(char_pool)))\n",
    "    for i_char, char in enumerate(word):\n",
    "        processed_word[0, i_char, char_indices[char]] = 1\n",
    "    prediction = model.predict(processed_word, verbose=0)[0]\n",
    "    \n",
    "    result = {'turk': prediction[0], 'ing': prediction[1]}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toprak: {'turk': 0.92372286, 'ing': 0.076277196}\n",
      "enginar: {'turk': 0.28950188, 'ing': 0.71049809}\n",
      "ornitorenk: {'turk': 0.025209611, 'ing': 0.97479033}\n",
      "frontier: {'turk': 0.000279842, 'ing': 0.99972016}\n",
      "television: {'turk': 0.00035036309, 'ing': 0.99964964}\n",
      "facebook: {'turk': 0.00049878447, 'ing': 0.99950123}\n",
      "ahahahah: {'turk': 0.98772639, 'ing': 0.012273617}\n",
      "xtr: {'turk': 0.00017934592, 'ing': 0.99982065}\n",
      "rabara: {'turk': 0.72516716, 'ing': 0.2748329}\n",
      "fizyoloji: {'turk': 0.99369228, 'ing': 0.0063076839}\n",
      "physiology: {'turk': 0.00017088205, 'ing': 0.99982905}\n"
     ]
    }
   ],
   "source": [
    "# [!] kelimelerin kucuk harflerle yazilmis olmasi gerekiyor.\n",
    "word_list = [\n",
    "    'toprak',\n",
    "    'enginar',\n",
    "    'ornitorenk',\n",
    "    'frontier',\n",
    "    'television',\n",
    "    'facebook',\n",
    "    # anlamsiz kelimeler\n",
    "    'ahahahah',\n",
    "    'xtr',\n",
    "    'rabara',\n",
    "    'fizyoloji',\n",
    "    'physiology'\n",
    "]\n",
    "\n",
    "for word in word_list:\n",
    "    prediction = predict(word)\n",
    "    print('{}: {}'.format(word, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonuclar\n",
    "* x, q ve w gibi Ingilice'ye has karakterlerin varligini kavramis\n",
    "* Turkcede -loji, Ingilizce'de -logy gibi eklerin kelime siniflandirmasinda onemli yer tuttugunu anlamis.\n",
    "* Gülmek daha Türkçemsi bir eylemmiş..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
